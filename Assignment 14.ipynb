{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c9a8bf-1288-4a30-8360-cac8ba90a986",
   "metadata": {},
   "source": [
    "# Task#01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a96e9b52-77c3-481b-8833-8577727d61fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e172692-e7af-4343-a73b-755d41c29cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_largest_companies_in_the_United_States_by_revenue'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd8c1b9e-4f01-4645-90f5-cf0b340d1788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syeda_ojd0o2c\\AppData\\Local\\Temp\\ipykernel_8132\\3135242424.py:4: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "C:\\Users\\syeda_ojd0o2c\\AppData\\Local\\Temp\\ipykernel_8132\\3135242424.py:4: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "C:\\Users\\syeda_ojd0o2c\\AppData\\Local\\Temp\\ipykernel_8132\\3135242424.py:4: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    }
   ],
   "source": [
    "tables = soup.find_all('table', {'class': 'wikitable'})\n",
    "\n",
    "for i, table in enumerate(tables):\n",
    "    df = pd.read_html(str(table))[0]\n",
    "    df.to_csv(f'table_{i+1}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efc3933-914f-4ce0-b095-450b597b867a",
   "metadata": {},
   "source": [
    "# Task#02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04b4be5d-5e6e-4c4d-8de1-e84cf7a7d2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Data scraping completed. Data saved to scraped_data.csv.\n"
     ]
    }
   ],
   "source": [
    "# Base URL with 100 rows per page\n",
    "base_url = \"https://www.scrapethissite.com/pages/forms/?per_page=100&page={}\"\n",
    "\n",
    "# List to store all data\n",
    "all_data = []\n",
    "\n",
    "# Define headers\n",
    "headers = [\"Team Name\",\"Year\", \"Wins\", \"Losses\", \"OT Losses\", \"Win %\", \"Goals For (GF\", \"Goals Against\", \"Plus/Minus\"]\n",
    "\n",
    "# Loop through a fixed range of pages (adjust the range based on the website)\n",
    "for page in range(1, 7):\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    url = base_url.format(page)  # Insert the page number into the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the page content with Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the table\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "    \n",
    "    # Extract rows from the table\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows[1:]:\n",
    "        cells = row.find_all('td')\n",
    "        if cells:\n",
    "            all_data.append([cell.text.strip() for cell in cells])\n",
    "\n",
    "    # Optional: Add a delay to avoid overloading the server\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df = pd.DataFrame(all_data, columns=headers)\n",
    "df.to_csv(\"scraped_data.csv\", index=False)\n",
    "\n",
    "print(\"Data scraping completed. Data saved to scraped_data.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bc7acc-f168-49a2-9b23-1f70d90f3b90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
